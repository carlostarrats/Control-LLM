import Foundation

// Llama.cpp bridge functions (placeholder implementation)
private func llama_load_model(_ model_path: UnsafePointer<CChar>) -> UnsafeMutableRawPointer? {
    print("üîç DEBUG: llama_load_model called with path: \(String(cString: model_path))")
    return UnsafeMutableRawPointer(bitPattern: 0x12345678)
}

private func llama_free_model(_ model: UnsafeMutableRawPointer) {
    print("üîç DEBUG: llama_free_model called")
}

private func llama_create_context(_ model: UnsafeMutableRawPointer) -> UnsafeMutableRawPointer? {
    print("üîç DEBUG: llama_create_context called")
    return UnsafeMutableRawPointer(bitPattern: 0x87654321)
}

private func llama_free_context(_ context: UnsafeMutableRawPointer) {
    print("üîç DEBUG: llama_free_context called")
}

private func llama_generate_text(_ context: UnsafeMutableRawPointer, _ prompt: UnsafePointer<CChar>, _ output: UnsafeMutablePointer<CChar>, _ max_length: Int32) -> Int32 {
    let promptString = String(cString: prompt)
    print("üîç DEBUG: llama_generate_text called with prompt: \(promptString)")
    
    // Simulate a more realistic response
    let response = "This is a simulated response from the llama.cpp integration. The prompt was: '\(promptString)'. In a real implementation, this would be generated by the actual Llama 3.2 model."
    
    let responseLength = min(Int32(response.count), max_length - 1)
    response.withCString { cString in
        strncpy(output, cString, Int(responseLength))
    }
    output[Int(responseLength)] = 0
    
    return responseLength
}

private func llama_generate_token(_ context: UnsafeMutableRawPointer, _ token: UnsafeMutablePointer<CChar>, _ max_token_length: Int32) -> Int32 {
    print("üîç DEBUG: llama_generate_token called")
    let dummyToken = "token"
    let tokenLength = min(Int32(dummyToken.count), max_token_length - 1)
    dummyToken.withCString { cString in
        strncpy(token, cString, Int(tokenLength))
    }
    token[Int(tokenLength)] = 0
    return tokenLength
}

private func llama_reset_context(_ context: UnsafeMutableRawPointer) {
    print("üîç DEBUG: llama_reset_context called")
}

private func llama_get_context_size(_ context: UnsafeMutableRawPointer) -> Int32 {
    print("üîç DEBUG: llama_get_context_size called")
    return 1024
}

final class LLMService {
    static let shared = LLMService()
    private var modelPath: String?
    private var currentModelFilename: String?
    private var llamaModel: UnsafeMutableRawPointer?
    private var llamaContext: UnsafeMutableRawPointer?
    private var isModelLoaded = false

    /// Load the currently selected model from ModelManager
    func loadSelectedModel() async throws {
        print("üö® LLMService: loadSelectedModel started")
        NSLog("üö® LLMService: loadSelectedModel started")
        fflush(stdout)
        
        guard let modelFilename = await ModelManager.shared.getSelectedModelFilename() else {
            print("üö® LLMService: ERROR - No model selected")
            NSLog("üö® LLMService: ERROR - No model selected")
            fflush(stdout)
            throw NSError(domain: "LLMService",
                          code: 1,
                          userInfo: [NSLocalizedDescriptionKey: "No model selected"])
        }
        
        // Don't reload if it's the same model
        if currentModelFilename == modelFilename && isModelLoaded {
            return
        }
        
        // Try to find the model file anywhere in the bundle resources
        var url: URL?
        if let allModelUrls = Bundle.main.urls(forResourcesWithExtension: "gguf", subdirectory: nil) {
            url = allModelUrls.first { candidate in
                candidate.lastPathComponent == "\(modelFilename).gguf"
            }
        }

        // Fallback: look directly at the bundle root path
        if url == nil {
            let rootUrl = Bundle.main.bundleURL.appendingPathComponent("\(modelFilename).gguf")
            if FileManager.default.fileExists(atPath: rootUrl.path) {
                url = rootUrl
            }
        }
        
        guard let modelUrl = url else {
            throw NSError(domain: "LLMService",
                          code: 1,
                          userInfo: [NSLocalizedDescriptionKey: "Model \(modelFilename).gguf not found in bundle or root directory"])
        }

        // Clear previous model to free memory
        unloadModel()
        
        self.modelPath = modelUrl.path
        self.currentModelFilename = modelFilename
        
        // Load the model using llama.cpp
        try await loadModelWithLlamaCpp()
        
        print("LLMService: Successfully loaded model \(modelFilename)")
    }
    
    private func unloadModel() {
        if let context = llamaContext {
            // Free llama.cpp context
            llama_free_context(context)
            llamaContext = nil
        }
        if let model = llamaModel {
            // Free llama.cpp model
            llama_free_model(model)
            llamaModel = nil
        }
        isModelLoaded = false
        modelPath = nil
        currentModelFilename = nil
    }
    
    private func loadModelWithLlamaCpp() async throws {
        guard let modelPath = modelPath else {
            throw NSError(domain: "LLMService", code: 3, userInfo: [NSLocalizedDescriptionKey: "No model path"])
        }
        
        // Load the model using llama.cpp (avoid force-unwrap)
        llamaModel = nil
        modelPath.withCString { cString in
            llamaModel = llama_load_model(cString)
        }
        
        guard let model = llamaModel else {
            throw NSError(domain: "LLMService", code: 4, userInfo: [NSLocalizedDescriptionKey: "Failed to load model"])
        }
        
        // Create context from model
        llamaContext = llama_create_context(model)
        
        guard let context = llamaContext else {
            throw NSError(domain: "LLMService", code: 5, userInfo: [NSLocalizedDescriptionKey: "Failed to create context"])
        }
        
        isModelLoaded = true
        print("LLMService: Model loaded successfully with llama.cpp")
    }
    
    private func withTimeout<T>(seconds: TimeInterval, operation: @escaping () async throws -> T) async throws -> T {
        return try await withThrowingTaskGroup(of: T.self) { group in
            group.addTask {
                try await operation()
            }
            
            group.addTask {
                try await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
                throw NSError(domain: "LLMService", code: 7, userInfo: [NSLocalizedDescriptionKey: "Model loading timed out"])
            }
            
            guard let result = try await group.next() else {
                group.cancelAll()
                throw NSError(domain: "LLMService", code: 8, userInfo: [NSLocalizedDescriptionKey: "No result from timeout operation"])
            }
            
            group.cancelAll()
            return result
        }
    }

    /// Stream tokens for a user prompt
    func chat(user text: String,
              onToken: @escaping (String) async -> Void) async throws {

        print("üö® LLMService: chat started with text: '\(text)'")
        NSLog("üö® LLMService: chat started with text: '\(text)'")
        fflush(stdout)
        
        print("üö® LLMService: About to load model")
        NSLog("üö® LLMService: About to load model")
        fflush(stdout)
        
        try await loadSelectedModel()
        
        print("üö® LLMService: Model loaded, isModelLoaded = \(isModelLoaded)")
        NSLog("üö® LLMService: Model loaded, isModelLoaded = \(isModelLoaded)")
        fflush(stdout)
        
        guard isModelLoaded, let context = llamaContext else {
            print("üö® LLMService: ERROR - Model not loaded")
            NSLog("üö® LLMService: ERROR - Model not loaded")
            fflush(stdout)
            throw NSError(domain: "LLMService",
                          code: 2,
                          userInfo: [NSLocalizedDescriptionKey: "Model not loaded"])
        }

        print("üö® LLMService: About to start prediction")
        NSLog("üö® LLMService: About to start prediction")
        fflush(stdout)
        
        do {
            // Add timeout protection for the prediction
            try await withTimeout(seconds: 30) { [self] in

                // Force console output with multiple approaches
                print("üîç DEBUG: User message: '\(text)'")
                NSLog("üîç DEBUG: User message: '\(text)'")
                fflush(stdout)
                
                print("üîç DEBUG: System prompt: '\(LanguageService.shared.getSystemPrompt())'")
                NSLog("üîç DEBUG: System prompt: '\(LanguageService.shared.getSystemPrompt())'")
                fflush(stdout)
                
                print("üîç DEBUG: Model filename: \(currentModelFilename ?? "unknown")")
                NSLog("üîç DEBUG: Model filename: \(currentModelFilename ?? "unknown")")
                fflush(stdout)
                
                // Build the prompt in the correct Llama 3.2 format
                let systemPrompt = LanguageService.shared.getSystemPrompt()
                let prompt = "<|begin_of_text|><|system|>\n\(systemPrompt)\n<|end|><|user|>\n\(text)\n<|end|><|assistant|>"
                
                print("üîç DEBUG: Full prompt: '\(prompt)'")
                
                // Use llama.cpp to generate response
                let response = try await generateResponseWithLlamaCpp(context: context, prompt: prompt, onToken: onToken)
                
                print("üîç DEBUG: Generated response: '\(response)'")
            }
        } catch {
            // Provide a helpful error message if the LLM fails
            await onToken("I'm having trouble responding right now. Please try again.")
            throw error
        }
    }
    
    private func generateResponseWithLlamaCpp(context: UnsafeMutableRawPointer, prompt: String, onToken: @escaping (String) async -> Void) async throws -> String {
        print("üîç DEBUG: Using llama.cpp bridge for text generation")
        
        // Convert prompt to C string
        let promptCString = prompt.cString(using: .utf8)!
        
        // Generate text using llama.cpp
        let maxOutputLength = 2048
        let outputBuffer = UnsafeMutablePointer<CChar>.allocate(capacity: maxOutputLength)
        defer { outputBuffer.deallocate() }
        
        let responseLength = llama_generate_text(context, promptCString, outputBuffer, Int32(maxOutputLength))
        
        if responseLength > 0 {
            let response = String(cString: outputBuffer)
            print("üîç DEBUG: Generated response: '\(response)'")
            
            // Stream the response word by word with proper spacing
            let words = response.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty }
            var tokenCount = 0
            for (index, word) in words.enumerated() {
                // Add space before each word except the first one
                let token = index == 0 ? word : " \(word)"
                await onToken(token)
                tokenCount += 1
            }
            print("üîç DEBUG: Response finished, total tokens =", tokenCount)
            
            return response
        } else {
            throw NSError(domain: "LLMService", code: 6, userInfo: [NSLocalizedDescriptionKey: "Failed to generate response"])
        }
    }
    
    /// Clear any potential state to ensure fresh calls
    func clearState() {
        unloadModel()
    }
}




